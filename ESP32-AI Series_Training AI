{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UI5fWfXWd3MM"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","# --- CONFIGURATION ---\n","# Generate file names from DataAI_So_0.csv to DataAI_So_9.csv\n","file_names = [f'DataAI_Number_{i}.csv' for i in range(10)]\n","# CHANGEABLE: Select which sample index to visualize (e.g., from 1 to 40)\n","sample_to_view = 1\n","rows_per_sample = 100\n","\n","# Create a figure with 10 subplots (5 rows x 2 columns)\n","fig, axes = plt.subplots(5, 2, figsize=(20, 25))\n","axes = axes.flatten()\n","\n","for i, file in enumerate(file_names):\n","    if os.path.exists(file):\n","        # 1. Load data\n","        df = pd.read_csv(file)\n","        df.columns = df.columns.str.strip() # Remove leading/trailing whitespaces from headers\n","\n","        # 2. Calculate row indices based on the selected sample\n","        start_row = (sample_to_view - 1) * rows_per_sample\n","        end_row = start_row + rows_per_sample\n","        data_slice = df.iloc[start_row:end_row]\n","\n","        # 3. Plot 6 components (ax, ay, az, gx, gy, gz)\n","        axes[i].plot(data_slice['ax'].values, label='ax', linewidth=1.5)\n","        axes[i].plot(data_slice['ay'].values, label='ay', linewidth=1.5)\n","        axes[i].plot(data_slice['az'].values, label='az', linewidth=1.5)\n","\n","        # Gyro values are usually much larger than Accel; scaled by 100 and use dashed lines for clarity\n","        axes[i].plot(data_slice['gx'].values / 100, label='gx/100', linestyle='--')\n","        axes[i].plot(data_slice['gy'].values / 100, label='gy/100', linestyle='--')\n","        axes[i].plot(data_slice['gz'].values / 100, label='gz/100', linestyle='--')\n","\n","        axes[i].set_title(f'Gesture: {file} (Sample Index: {sample_to_view})', fontsize=14, fontweight='bold')\n","        axes[i].legend(loc='upper right', fontsize='small', ncol=2)\n","        axes[i].grid(True, alpha=0.3)\n","    else:\n","        # Placeholder text if the data file is missing\n","        axes[i].text(0.5, 0.5, f'File not found: {file}', ha='center')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jAjF1CW1IlFo"},"source":["# **CONCATENATE ALL DATA FILES**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN0U1pT-Iyad"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Lists to store processed features and labels\n","X_list = []\n","y_list = []\n","\n","# Iterate through 10 files from DataAI_So_0.csv to DataAI_So_9.csv\n","for i in range(10):\n","    file_name = f'DataAI_Number_{i}.csv'\n","    try:\n","        # Load file (Pandas uses the first row as header; 4000 rows of data remain)\n","        df = pd.read_csv(file_name)\n","        df.columns = df.columns.str.strip() # Remove whitespaces from column headers\n","\n","        # 1. Extract 6 sensor columns (Features)\n","        sensor_data = df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values\n","\n","        # 2. Reshape: Convert 4000 rows into 40 samples (Each sample: 100 rows x 6 axes)\n","        # Target shape: (40 samples, 100 timesteps, 6 features)\n","        samples = sensor_data.reshape(40, 100, 6)\n","        X_list.append(samples)\n","\n","        # 3. Extract labels from 'label' column (Take the first value of each sample)\n","        # Since every 100 rows share the same label, we pick values at indices 0, 100, 200...\n","        labels = df['label'].values[::100]\n","        y_list.append(labels)\n","\n","        print(f\"Successfully processed {file_name}\")\n","    except Exception as e:\n","        print(f\"Error processing {file_name}: {e}\")\n","\n","# Combine all lists into large Numpy arrays\n","X_final = np.vstack(X_list)      # Final shape: (400, 100, 6)\n","y_final = np.concatenate(y_list) # Final shape: (400,)\n","\n","print(\"-\" * 30)\n","print(f\"Total Samples: {X_final.shape[0]}\")\n","print(f\"Sample Length (Timesteps): {X_final.shape[1]}\")\n","print(f\"Sensor Axes (Features): {X_final.shape[2]}\")"]},{"cell_type":"markdown","metadata":{"id":"CoeaaPzsI8AN"},"source":["# **SPLIT DATASET INTO TRAIN, VALIDATION, AND TEST SETS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYEDsR5TI94v"},"outputs":[],"source":["# Step 1: Set aside 5% for the final Test set\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    X_final, y_final,\n","    test_size=0.05,\n","    random_state=42,\n","    stratify=y_final, # Ensure balanced class distribution (0-9)\n","    shuffle=True\n",")\n","\n","# Step 2: Split the remaining 95% into Training (80%) and Validation (20%)\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp,\n","    test_size=0.2,\n","    random_state=42,\n","    stratify=y_temp,\n","    shuffle=True\n",")\n","\n","print(f\"Training set size:   {X_train.shape} - {len(y_train)} labels\")\n","print(f\"Validation set size: {X_val.shape} - {len(y_val)} labels\")\n","print(f\"Test set size:       {X_test.shape} - {len(y_test)} labels\")"]},{"cell_type":"markdown","metadata":{"id":"cVDCGnWUIx48"},"source":["# **MAIN MODEL TRAINING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8bjByI4I0F8"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","import seaborn as sns\n","\n","# --- SUPPLEMENTARY STEP: DATA NORMALIZATION ---\n","# Calculate mean and std from the Training set to normalize all datasets (Z-score)\n","mean = X_train.mean(axis=(0, 1))\n","std = X_train.std(axis=(0, 1))\n","\n","X_train_norm = (X_train - mean) / std\n","X_val_norm = (X_val - mean) / std\n","X_test_norm = (X_test - mean) / std\n","\n","# --- MODEL TRAINING WITH HYPERPARAMETER TUNING ---\n","lambdas = [0.005, 0.007, 0.009, 0.01, 0.013, 0.015, 0.017]\n","models = [None] * len(lambdas)\n","\n","for i in range(len(lambdas)):\n","    _lambda = lambdas[i]\n","    models[i] = Sequential([\n","        # Flatten (100, 6) input into a 600-element vector\n","        Flatten(input_shape=(100, 6)),\n","\n","        Dense(units=40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(_lambda)),\n","        Dense(units=32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(_lambda)),\n","        Dense(units=16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(_lambda)),\n","        Dense(units=10, activation='linear'), # Output 10 classes (0-9)\n","    ])\n","\n","    models[i].compile(\n","        loss=SparseCategoricalCrossentropy(from_logits=True),\n","        optimizer=Adam(learning_rate=0.001), # Increased learning rate for faster convergence\n","        metrics=['accuracy']\n","    )\n","\n","    print(f\"\\n--- Training model with lambda = {_lambda} ---\")\n","    models[i].fit(\n","        X_train_norm, y_train,\n","        epochs=150,\n","        validation_data=(X_val_norm, y_val), # Monitor Validation set for Overfitting\n","        verbose=1\n","    )\n","\n","# --- MODEL EVALUATION ---\n","# =============================================================================\n","# STEP 1: PERFORMANCE EVALUATION\n","# =============================================================================\n","\n","train_losses = []\n","val_losses = []\n","val_accuracies = []\n","\n","print(\"\\n\" + \"=\"*40)\n","print(f\"{'Lambda':<10} | {'Train Loss':<12} | {'Val Loss':<10} | {'Val Acc':<10}\")\n","print(\"-\" * 50)\n","\n","for i in range(len(lambdas)):\n","    # Evaluate returns Loss and Accuracy\n","    t_loss, t_acc = models[i].evaluate(X_train_norm, y_train, verbose=0)\n","    v_loss, v_acc = models[i].evaluate(X_val_norm, y_val, verbose=0)\n","\n","    train_losses.append(t_loss)\n","    val_losses.append(v_loss)\n","    val_accuracies.append(v_acc)\n","\n","    print(f\"{lambdas[i]:<10.3f} | {t_loss:<12.4f} | {v_loss:<10.4f} | {v_acc * 100:.2f}%\")\n","\n","# =============================================================================\n","# STEP 2: BIAS-VARIANCE ANALYSIS & BEST MODEL SELECTION\n","# =============================================================================\n","\n","# Plot Bias - Variance Tradeoff (Goal: Find the minimum Validation Loss)\n","plt.figure(figsize=(10, 5))\n","plt.plot(lambdas, train_losses, 'o-', label='Train Loss (J_train)', color='blue')\n","plt.plot(lambdas, val_losses, 'o-', label='Val Loss (J_cv)', color='orange')\n","plt.title('BIAS - VARIANCE TRADEOFF (Optimal Lambda Selection)')\n","plt.xlabel('Lambda (Regularization Strength)')\n","plt.ylabel('Loss (Cost)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Automatically select the model with the lowest Validation Loss\n","best_idx = np.argmin(val_losses)\n","best_lambda = lambdas[best_idx]\n","best_model = models[best_idx]\n","\n","print(f\"\\n===> CONCLUSION: Best model found at index {best_idx} with Lambda = {best_lambda}\")\n","print(f\"     Lowest Val Loss: {val_losses[best_idx]:.4f}\")\n","print(f\"     Validation Accuracy: {val_accuracies[best_idx]*100:.2f}%\")\n","\n","# =============================================================================\n","# STEP 3: FINAL EVALUATION ON TEST SET (UNSEEN DATA)\n","# =============================================================================\n","print(\"\\n\" + \"=\"*40)\n","print(\"FINAL TEST SET EVALUATION\")\n","print(\"=\"*40)\n","\n","test_loss, test_acc = best_model.evaluate(X_test_norm, y_test, verbose=0)\n","print(f\"Test Set Accuracy: {test_acc * 100:.2f}%\")\n","\n","# Plot Confusion Matrix (To identify misclassifications)\n","y_pred_logits = best_model.predict(X_test_norm)\n","y_pred_labels = np.argmax(y_pred_logits, axis=1)\n","\n","cm = confusion_matrix(y_test, y_pred_labels)\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('AI Predicted')\n","plt.ylabel('Actual')\n","plt.title(f'Confusion Matrix - Test Acc: {test_acc*100:.1f}%')\n","plt.show()\n","\n","# =============================================================================\n","# STEP 4: MODEL EXPORT FOR ESP32 (TFLITE & HEADER FILE)\n","# =============================================================================\n","print(\"\\n\" + \"=\"*40)\n","# print(\"EXPORTING FILES FOR ARDUINO/ESP32\")\n","# print(\"=\"*40)\n","\n","# 1. Convert to TensorFlow Lite format\n","converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n","tflite_model = converter.convert()\n","\n","# 2. Save as .tflite file\n","with open('magic_wand_model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","print(\"-> Saved 'magic_wand_model.tflite'\")\n","\n","# 3. Generate C++ Header file (.h) containing the Hex array\n","import os\n","os.system('apt-get -qq install xxd')\n","os.system('xxd -i magic_wand_model.tflite > model_data.h')\n","print(\"-> Generated 'model_data.h'\")\n","\n","# =============================================================================\n","# STEP 5: NORMALIZATION PARAMETERS EXPORT (CRITICAL)\n","# =============================================================================\n","print(\"\\n\" + \"!\"*50)\n","print(\"!\"*50)\n","print(\"// These parameters are extracted from the Python training process\")\n","print(\"// Used to normalize raw sensor data before inference\")\n","print(\"const float MEAN[] = {\", \", \".join([f\"{x:.4f}\" for x in mean]), \"};\")\n","print(\"const float STD[]  = {\", \", \".join([f\"{x:.4f}\" for x in std]), \"};\")\n","print(\"!\"*50)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPwiejwFR52MkF2rzbqSOWe"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}